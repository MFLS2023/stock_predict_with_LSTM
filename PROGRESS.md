# AI动态网格交易优化方案 v2.0 (PPO-Based)

**最后更新**: 2025年11月8日 (已成功完成首次PPO模型训练)

---

### **核心目标 (Core Objective)**

训练一个强化学习（RL）智能体，使其能够根据实时的市场状态，动态地调整网格交易策略的关键参数，以实现在多种市场环境下稳定超越“定投”基准的收益表现。

### **核心思想 (Core Idea)**

我们将AI的角色从一个直接的“交易员”（执行买卖）转变为一个更高级的“策略分析师”（设定交易参数）。AI不进行微观的买卖决策，而是专注于在每个决策周期开始时，为底层的网格交易系统输出一套最优的“作战计划”（即网格参数）。

### **系统架构 (System Architecture) - v2.0 标准**

1.  **智能体 (The Agent - “大脑”)**
    *   **技术选型**: **PPO（Proximal Policy Optimization）** 算法，当前默认策略为 `MlpLstmPolicy`，能够记忆时间序列模式。
    *   **实现**: 通过集成 `stable-baselines3` 这个专业的强化学习库来完成。

2.  **状态空间 (State Space - “眼睛”)**
    *   **定义**: AI进行决策所依赖的全部市场信息。
    *   **构成**: 基于知行均线的特征体系，并包含投资组合的动态状态。

3.  **动作空间 (Action Space - “双手”)**
    *   **定义**: AI可以执行的操作，即输出一套风控驱动的组合指令。
    *   **构成**: 一个包含五个**连续值**的向量: `[buy_threshold, sell_threshold, target_position, stop_loss_pct, take_profit_pct]`，用于决定何时调仓以及新建仓位时的止盈/止损线。

4.  **环境 (Environment - “训练场”)**
    *   **定义**: 模拟真实交易，并为AI提供反馈的虚拟世界。
    *   **构成**: `ai_strategy.py` 中符合 `gymnasium` 标准的自定义环境 `DynamicGridEnv`，支持动态止盈止损、风险距离观测以及与定投基准对比的奖励机制。

5.  **奖励函数 (Reward Function - “计分板”)**
    *   **定义**: 评估AI动作好坏的唯一标准。
    *   **构成**: 以最大化“超额收益”并最小化“交易成本”为基础，同时针对触发止盈给予额外正奖励、触发止损给予额外惩罚，引导策略在盈利时让利润奔跑、亏损时快速脱身。

---

### **实施路线图 (Implementation Roadmap)**

**第一阶段：基础建设与特征工程**
*   **状态**: `已完成`

**第二阶段：搭建RL环境**
*   **状态**: `已完成`

**第三阶段：AI智能体设计与训练**
*   **状态**: `已完成`
*   **任务**:
    *   [X] **3.1 & 3.2**: 已有的特征工程和环境类为重构提供了良好基础。
    *   [X] **3.3. 拨乱反正**: 成功决策并回归 v2.0 (PPO) 方案。
    *   [X] **3.4. 重构环境与训练脚本**: 已根据 v2.0 方案，重构 `ai_strategy.py` 和 `train_agent.py`。
    *   [X] **3.5. 启动并完成一次有意义的训练**: 已成功完成首次PPO模型训练并保存了模型。

**第四阶段：评估、对比与分析**
*   **状态**: `进行中`
*   **任务**:
    *   [ ] **4.1. 评估与分析 (当前步骤)**: 分析首次训练的结果，并根据“训练时间过长”的反馈优化训练流程。
    *   [ ] **4.2. 实现评估体系**: 计算超额收益、最大回撤、夏普比率等关键指标。
    *   [ ] **4.3. 结果可视化**: 绘制AI动态网格、静态网格、定投策略的收益对比曲线。

**第五阶段：自适应进化与调参引擎**
*   **状态**: `启动`
*   **任务**:
    *   [X] **5.1. Optuna 调参脚本**: 新增 `tune.py`，支持自动化搜索学习率、n_steps、LSTM 隐藏维度等关键超参数。
    *   [ ] **5.2. 批量实验管理**: 基于 Study Storage 持续积累试验结果，形成可复现的调参工作流。
    *   [ ] **5.3. 最优参数回放**: 将最优参数回写到正式训练流程，形成迭代闭环。

---