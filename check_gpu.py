"""
GPU è¯Šæ–­å’Œæµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯PyTorch GPUå®‰è£…æ˜¯å¦æ­£ç¡®
"""

import sys

def check_pytorch_gpu():
    print("=" * 60)
    print("PyTorch GPU è¯Šæ–­å·¥å…·")
    print("=" * 60)
    
    # 1. æ£€æŸ¥PyTorchæ˜¯å¦å®‰è£…
    try:
        import torch
        print(f"âœ… PyTorchå·²å®‰è£…")
        print(f"   ç‰ˆæœ¬: {torch.__version__}")
    except ImportError:
        print("âŒ PyTorchæœªå®‰è£…ï¼")
        print("   è¯·è¿è¡Œ: pip install torch torchvision torchaudio")
        return False
    
    # 2. æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
    cuda_available = torch.cuda.is_available()
    if cuda_available:
        print(f"âœ… CUDAå¯ç”¨")
        print(f"   CUDAç‰ˆæœ¬: {torch.version.cuda}")
    else:
        print(f"âŒ CUDAä¸å¯ç”¨")
        if '+cpu' in torch.__version__:
            print(f"   åŸå› : ä½ å®‰è£…çš„æ˜¯CPUç‰ˆæœ¬çš„PyTorch ({torch.__version__})")
            print(f"   è§£å†³æ–¹æ¡ˆ: è¯·æŸ¥çœ‹ INSTALL_PYTORCH_GPU.md é‡æ–°å®‰è£…")
        else:
            print(f"   åŸå› : æœªçŸ¥ï¼ˆå¯èƒ½æ˜¯é©±åŠ¨é—®é¢˜ï¼‰")
        return False
    
    # 3. æ£€æŸ¥GPUè®¾å¤‡
    gpu_count = torch.cuda.device_count()
    print(f"âœ… æ£€æµ‹åˆ° {gpu_count} ä¸ªGPUè®¾å¤‡")
    
    for i in range(gpu_count):
        gpu_name = torch.cuda.get_device_name(i)
        gpu_capability = torch.cuda.get_device_capability(i)
        gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
        print(f"\n   GPU {i}:")
        print(f"     åç§°: {gpu_name}")
        print(f"     è®¡ç®—èƒ½åŠ›: {gpu_capability[0]}.{gpu_capability[1]}")
        print(f"     æ˜¾å­˜: {gpu_memory:.2f} GB")
    
    # 4. æµ‹è¯•GPUè®¡ç®—
    print("\n" + "=" * 60)
    print("GPU è®¡ç®—æµ‹è¯•")
    print("=" * 60)
    
    try:
        import time
        
        # CPUæµ‹è¯•
        size = 5000
        a_cpu = torch.randn(size, size)
        b_cpu = torch.randn(size, size)
        
        start = time.perf_counter()
        c_cpu = torch.matmul(a_cpu, b_cpu)
        cpu_time = time.perf_counter() - start
        print(f"CPUè®¡ç®— ({size}x{size} çŸ©é˜µä¹˜æ³•): {cpu_time:.4f} ç§’")
        
        # GPUæµ‹è¯•
        device = torch.device("cuda:0")
        a_gpu = a_cpu.to(device)
        b_gpu = b_cpu.to(device)
        
        # é¢„çƒ­
        _ = torch.matmul(a_gpu, b_gpu)
        torch.cuda.synchronize()
        
        start = time.perf_counter()
        c_gpu = torch.matmul(a_gpu, b_gpu)
        torch.cuda.synchronize()
        gpu_time = time.perf_counter() - start
        
        print(f"GPUè®¡ç®— ({size}x{size} çŸ©é˜µä¹˜æ³•): {gpu_time:.4f} ç§’")
        print(f"ğŸš€ åŠ é€Ÿæ¯”: {cpu_time/gpu_time:.2f}x")
        
        # éªŒè¯ç»“æœä¸€è‡´æ€§
        c_gpu_cpu = c_gpu.cpu()
        diff = torch.abs(c_cpu - c_gpu_cpu).max().item()
        print(f"âœ… ç»“æœå·®å¼‚: {diff:.2e} (åº”è¯¥æ¥è¿‘0)")
        
    except Exception as e:
        print(f"âŒ GPUè®¡ç®—æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    # 5. æ£€æŸ¥cuDNN
    print("\n" + "=" * 60)
    print("cuDNN çŠ¶æ€")
    print("=" * 60)
    cudnn_available = torch.backends.cudnn.enabled
    if cudnn_available:
        print(f"âœ… cuDNNå·²å¯ç”¨")
        print(f"   ç‰ˆæœ¬: {torch.backends.cudnn.version()}")
    else:
        print(f"âš ï¸  cuDNNæœªå¯ç”¨")
    
    print("\n" + "=" * 60)
    print("âœ… æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼PyTorch GPUé…ç½®æ­£ç¡®")
    print("=" * 60)
    return True


if __name__ == "__main__":
    success = check_pytorch_gpu()
    sys.exit(0 if success else 1)
